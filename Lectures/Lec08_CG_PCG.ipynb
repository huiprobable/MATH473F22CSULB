{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44af02aa-c7c4-4366-b4bf-a741e3141766",
   "metadata": {},
   "source": [
    "# Conjugate Gradient and Preconditioned Conjugate Gradient methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad97252e-6a70-4c04-bb6e-0e0bdd2b333a",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "\n",
    "Stationary methods\n",
    "\n",
    "* Jacobi method\n",
    "\n",
    "$$x^{(k+1)}_i = \\frac{1}{A_{i,i}}\\left(b_i-\\sum_{j\\neq i}A_{i,j}x^{(k)}_j\\right)$$\n",
    "\n",
    "In vectorized form\n",
    "$${\\bf x}^{(k+1)} = D^{-1}({\\bf b}-(A-D){\\bf x}^{(k)})$$\n",
    "\n",
    "* Gauss Seidel method\n",
    "\n",
    "$$x^{(k+1)}_i = \\frac{1}{A_{i,i}}\\left(b_i-\\sum_{j<i}A_{i,j}x^{(k+1)}_j-\\sum_{j>i}A_{i,j}x^{(k)}_j\\right)$$\n",
    "\n",
    "In vectorized form\n",
    "$${\\bf x}^{(k+1)} = L^{-1}({\\bf b}-U{\\bf x}^{(k)})$$\n",
    "\n",
    "* Over Successive Relaxation \n",
    "\n",
    "$$x^{(k+1)}_i = \\omega\\frac{1}{A_{i,i}}\\left(b_i-\\sum_{j<i}A_{i,j}x^{(k+1)}_j-\\sum_{j>i}A_{i,j}x^{(k)}_j\\right) + (1-\\omega)x^{(k)}_i$$\n",
    "\n",
    "In vectorized form\n",
    "$${\\bf x}^{(k+1)} = \\omega L^{-1}({\\bf b}-U{\\bf x}^{(k)}) + (1-\\omega){\\bf x}^{(k)}=\\omega{\\bf x}^{(k+1)}_{GS}+(1-\\omega){\\bf x}^{(k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28741e40-9119-4f16-b9f3-30636523fd5a",
   "metadata": {},
   "source": [
    "These methods can all be summarized as: (*stationary method*)\n",
    "\n",
    "$${\\bf x}^{(k+1)} = {\\bf x}^{(k)}+M^{-1}({\\bf b}-A{\\bf x}^{(k)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db257908",
   "metadata": {},
   "source": [
    "* Jacobi:\n",
    "\\begin{align}\n",
    "{\\bf x}^{(k+1)} &= D^{-1}({\\bf b}-(A-D){\\bf x}^{(k)})\\\\\n",
    "{\\bf x}^{(k+1)} &= D^{-1}({\\bf b}-A{\\bf x}^{(k)}+D{\\bf x}^{(k)})\\\\\n",
    "{\\bf x}^{(k+1)} &= {\\bf x}^{(k)}+D^{-1}({\\bf b}-A{\\bf x}^{(k)})\n",
    "\\end{align}\n",
    "\n",
    "* Gauss-Seidel:\n",
    "\\begin{align}\n",
    "{\\bf x}^{(k+1)} &= L^{-1}({\\bf b}-U{\\bf x}^{(k)})\\\\\n",
    "{\\bf x}^{(k+1)} &= L^{-1}({\\bf b}-(U+L-L){\\bf x}^{(k)})\\\\\n",
    "{\\bf x}^{(k+1)} &= L^{-1}({\\bf b}-(A-L){\\bf x}^{(k)})\\\\\n",
    "{\\bf x}^{(k+1)} &= {\\bf x}^{(k)}+L^{-1}({\\bf b}-A{\\bf x}^{(k)})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ff27d-eb85-4958-9212-545e6e8d679e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(256, 256, 0, 0.1); padding:10px 0;font-family:monospace;\">\n",
    "    <b>Theorem: Stationary Method Convergence.</b><br><hr>\n",
    "    For the linear problem $A{\\bf x}={\\bf b}$, consider the iterative method $${\\bf x}^{(k+1)}={\\bf x}^{(k)}+M^{-1}{\\bf r}^{(k)},$$ where ${\\bf r}^{(k)}={\\bf b}-A{\\bf x}^{(k)}$ is the residue at k-th step, and define the <b>iteration matrix</b> $T=I-M^{-1}A$. Then the method converges if and only if the spectral radius of the iteration matrix satisfies $$\\rho(T)<1$$ The smaller $\\rho(T)$, the faster the convergence. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ebea79",
   "metadata": {},
   "source": [
    "#### Proof of the theorem:\n",
    "Let $x^*$ be the exact solution, so $A{\\bf x}^*={\\bf b}$, and ${\\bf r}^{(k)} = {\\bf b}-A{\\bf x}^{(k)}=A({\\bf x}^*-{\\bf x}^{(k)})$\n",
    "\\begin{align}\n",
    "{\\bf x}^{(k+1)}&={\\bf x}^{(k)}+M^{-1}{\\bf r}^{(k)}\\\\\n",
    "{\\bf x}^{(k+1)}-{\\bf x}^*&={\\bf x}^{(k)}-{\\bf x}^*+M^{-1}{\\bf r}^{(k)}\\\\\n",
    "{\\bf e}^{(k+1)} &= {\\bf e}^{(k)}-M^{-1}A{\\bf e}^{(k)}\\\\\n",
    "{\\bf e}^{(k+1)} &= (I-M^{-1}A){\\bf e}^{(k)}\\\\\n",
    "{\\bf e}^{(k+1)} &= T{\\bf e}^{(k)}\\\\\n",
    "\\end{align}\n",
    "From there, we get\n",
    "\\begin{align}\n",
    "\\|{\\bf e}^{(k+1)}\\|_2&\\leq \\|T\\|_2\\|{\\bf e}^{(k)}\\|_2\\\\\n",
    "&\\leq \\rho(T)\\|{\\bf e}^{(k)}\\|_2\\\\\n",
    "&\\leq \\rho(T)^2\\|{\\bf e}^{(k-1)}\\|_2\\\\\n",
    "&\\leq \\rho(T)^{k+1}\\|{\\bf e}^{(0)}\\|_2\n",
    "\\end{align}\n",
    "\n",
    "So, we know that $\\|{\\bf e}^{(k)}\\|_2=C\\rho(T)^{k}$<br>\n",
    "If you take $\\log_{10}$ from both sides, you can see that \n",
    "$$\\log_{10}(\\|{\\bf e}^{(k)}\\|_2) = k \\log_{10}(\\rho(T))+\\log_{10}C$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b5436d-56c6-4017-965f-5f027fa45e4a",
   "metadata": {},
   "source": [
    "### Conjugate Gradient Method\n",
    "\n",
    "Conjugate gradient method is derived from **steepest descent method**. So we need to first talk about steepest descent method. <br>\n",
    "Solving $A{\\bf x}={\\bf b}$ is equivalent to minimizing the following functional:\n",
    "$$\\phi({\\bf x})=\\frac{1}{2}{\\bf x}^TA{\\bf x}-{\\bf b}^T{\\bf x}$$\n",
    "Gradient descent iteration:\n",
    "$${\\bf x}^{(k+1)}={\\bf x}^{(k)}+\\alpha_k{\\bf r}^{(k)}$$\n",
    "Which leads to minimizing:\n",
    "$$\\frac{1}{2}({\\bf x}^{(k)}+\\alpha_k{\\bf r}^{(k)})^TA({\\bf x}^{(k)}+\\alpha_k{\\bf r}^{(k)})-{\\bf b}^T({\\bf x}^{(k)}+\\alpha_k{\\bf r}^{(k)})$$\n",
    "Taking derivative with respect to $\\alpha_k$, we get \n",
    "$$\\alpha_k{\\bf r}^{(k)T}A{\\bf r}^{(k)}+{\\bf r}^{(k)T}A{\\bf x}^{(k)}-{\\bf r}^{(k)T}{\\bf b}=0$$\n",
    "This leads us to \n",
    "$$\\alpha_k=\\dfrac{{\\bf r}^{(k)T}{\\bf r}^{(k)}}{{\\bf r}^{(k)T}A{\\bf r}^{(k)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2230cb94-3b76-4f07-ae4d-9a2196917751",
   "metadata": {},
   "source": [
    "The **Conjugatet Gradient Method** takes the search direction ${\\bf p}^{(k)}$ no longer the same as the residual vector ${\\bf r}^{(k)}$, but the search direction ${\\bf p}^{(k)}$ needs to be perpendicular to all the previous search directions ${\\bf p}^{(j)}$ for $j<k$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a225b-143e-40c5-8132-cd290d81c2e2",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(0, 256, 0, 0.1); padding:10px 0;font-family:monospace;\">\n",
    "    <b>Algorithm: Conjugate Gradient Method.</b><br><hr>\n",
    "    Given an initial guess ${\\bf x}_0$ and a tolerance $\\epsilon$, set at first ${\\bf r}_0={\\bf b}-A{\\bf x}_0$, $\\delta_0=\\langle {\\bf r}_0, {\\bf r}_0\\rangle$, ${\\bf b}_{\\delta} = \\langle {\\bf b}_0, {\\bf b}_0\\rangle$, $k=0$, and ${\\bf p}_0={\\bf r}_0$. Then:<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; while $\\delta_k>\\epsilon^2{\\bf b}_{\\delta}$:<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; ${\\bf s}_k=A{\\bf p}_k$<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; $\\alpha_k = \\dfrac{\\delta_k}{\\langle {\\bf p}_k, {\\bf s}_k\\rangle}$<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; ${\\bf x}_{k+1} = {\\bf x}_{k}+\\alpha_k{\\bf p}_k$<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; ${\\bf r}_{k+1} = {\\bf r}_{k}-\\alpha_k{\\bf s}_k$<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; $\\delta_{k+1} = \\langle {\\bf r}_{k+1}, {\\bf r}_{k+1}\\rangle$<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; ${\\bf p}_{k+1} = {\\bf r}_{k+1}+\\dfrac{\\delta_{k+1}}{\\delta_{k}} {\\bf p}_{k}$<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; $k = k+1$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2062905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CG(A, b, tol=1e-6):\n",
    "    x = np.zeros_like(b)\n",
    "    r = b - A@x\n",
    "    delta = r@r\n",
    "    delta_b = b@b\n",
    "    k = 0\n",
    "    p = r\n",
    "    while delta>tol*tol*delta_b:\n",
    "        s = A@p\n",
    "        alpha = delta/(p@s)\n",
    "        x += alpha*p\n",
    "        r -= alpha*s\n",
    "        delta_old = delta\n",
    "        delta = r@r\n",
    "        p = r+(delta/delta_old)*p\n",
    "        k += 1\n",
    "    return x, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f93eab86-cf14-4983-9a78-ad95a13127f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import dok_matrix\n",
    "def ind(i,j,n):\n",
    "    return i+j*(n+1)\n",
    "def formingSparseLinearSystem(n, g):\n",
    "    h = 1/n\n",
    "    A = dok_matrix(((n+1)**2,(n+1)**2), dtype = np.float64)\n",
    "    b = np.zeros((n+1)**2)\n",
    "    for i in range(n+1):\n",
    "        for j in range(n+1):\n",
    "            if 0<i<n and 0<j<n:\n",
    "                A[ind(i,j,n),ind(i,j,n)] = 4\n",
    "                A[ind(i,j,n),ind(i-1,j,n)] = -1\n",
    "                A[ind(i,j,n),ind(i+1,j,n)] = -1\n",
    "                A[ind(i,j,n),ind(i,j-1,n)] = -1\n",
    "                A[ind(i,j,n),ind(i,j+1,n)] = -1\n",
    "                b[ind(i,j,n)] = g(j*h,i*h)*h*h\n",
    "            else:\n",
    "                A[ind(i,j,n),ind(i,j,n)] = 1\n",
    "                b[ind(i,j,n)] = 0\n",
    "    A = A.tocsr()\n",
    "    return A, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96d1286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda x,y: x*(1-x)+y*(2-y)\n",
    "n = 30\n",
    "As, bs = formingSparseLinearSystem(n, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbe59528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CG(A, b, tol=1e-6):\n",
    "    x = np.zeros_like(b)\n",
    "    r = b - A@x\n",
    "    delta = r@r\n",
    "    delta_b = b@b\n",
    "    k = 0\n",
    "    p = r\n",
    "    while delta>tol*tol*delta_b:\n",
    "        s = A@p\n",
    "        alpha = delta/(p@s)\n",
    "        x += alpha*p\n",
    "        r -= alpha*s\n",
    "        delta_old = delta\n",
    "        delta = r@r\n",
    "        p = r+(delta/delta_old)*p\n",
    "        k += 1\n",
    "    return x, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0266e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, k = CG(As, bs,tol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c766a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "857"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71386a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb(k):\n",
    "    k = k+1\n",
    "#    print(k)\n",
    "    return k\n",
    "\n",
    "k = 0\n",
    "\n",
    "from scipy.sparse.linalg import cg\n",
    "x = cg(As, bs, callback = cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a096b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
