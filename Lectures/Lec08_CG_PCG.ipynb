{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44af02aa-c7c4-4366-b4bf-a741e3141766",
   "metadata": {},
   "source": [
    "# Conjugate Gradient and Preconditioned Conjugate Gradient methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad97252e-6a70-4c04-bb6e-0e0bdd2b333a",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "\n",
    "Stationary methods\n",
    "\n",
    "* Jacobi method\n",
    "\n",
    "$$x^{(k+1)}_i = \\frac{1}{A_{i,i}}\\left(b_i-\\sum_{j\\neq i}A_{i,j}x^{(k)}_j\\right)$$\n",
    "\n",
    "In vectorized form\n",
    "$${\\bf x}^{(k+1)} = D^{-1}({\\bf b}-(A-D){\\bf x}^{(k)})$$\n",
    "\n",
    "* Gauss Seidel method\n",
    "\n",
    "$$x^{(k+1)}_i = \\frac{1}{A_{i,i}}\\left(b_i-\\sum_{j<i}A_{i,j}x^{(k+1)}_j-\\sum_{j>i}A_{i,j}x^{(k)}_j\\right)$$\n",
    "\n",
    "In vectorized form\n",
    "$${\\bf x}^{(k+1)} = L^{-1}({\\bf b}-U{\\bf x}^{(k)})$$\n",
    "\n",
    "* Over Successive Relaxation \n",
    "\n",
    "$$x^{(k+1)}_i = \\omega\\frac{1}{A_{i,i}}\\left(b_i-\\sum_{j<i}A_{i,j}x^{(k+1)}_j-\\sum_{j>i}A_{i,j}x^{(k)}_j\\right) + (1-\\omega)x^{(k)}_i$$\n",
    "\n",
    "In vectorized form\n",
    "$${\\bf x}^{(k+1)} = \\omega L^{-1}({\\bf b}-U{\\bf x}^{(k)}) + (1-\\omega){\\bf x}^{(k)}=\\omega{\\bf x}^{(k+1)}_{GS}+(1-\\omega){\\bf x}^{(k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28741e40-9119-4f16-b9f3-30636523fd5a",
   "metadata": {},
   "source": [
    "These methods can all be summarized as: (*stationary method*)\n",
    "\n",
    "$${\\bf x}^{(k+1)} = {\\bf x}^{(k)}+M^{-1}({\\bf b}-A{\\bf x}^{(k)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ff27d-eb85-4958-9212-545e6e8d679e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(256, 256, 0, 0.1); padding:10px 0;font-family:monospace;\">\n",
    "    <b>Theorem: Stationary Method Convergence.</b><br><hr>\n",
    "    For the linear problem $A{\\bf x}={\\bf b}$, consider the iterative method $${\\bf x}^{(k+1)}={\\bf x}^{(k)}+M^{-1}{\\bf r}^{(k)}$$ and define the <b>iteration matrix</b> $T=I-M^{-1}A$. Then the method converges if and only if the spectral radius of the iteration matrix satisfies $$\\rho(T)<1$$ The smaller $\\rho(T)$, the faster the convergence. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b5436d-56c6-4017-965f-5f027fa45e4a",
   "metadata": {},
   "source": [
    "### Conjugate Gradient Method\n",
    "\n",
    "Conjugate gradient method is derived from **steepest descent method**. So we need to first talk about steepest descent method. <br>\n",
    "Solving $A{\\bf x}={\\bf b}$ is equivalent to minimizing the following functional:\n",
    "$$\\phi({\\bf x})=\\frac{1}{2}{\\bf x}^TA{\\bf x}-{\\bf b}^T{\\bf x}$$\n",
    "Gradient descent iteration:\n",
    "$${\\bf x}^{(k+1)}={\\bf x}^{(k)}+\\alpha_k{\\bf r}^{(k)}$$\n",
    "Which leads to minimizing:\n",
    "$$\\frac{1}{2}({\\bf x}^{(k)}+\\alpha_k{\\bf r}^{(k)})^TA({\\bf x}^{(k)}+\\alpha_k{\\bf r}^{(k)})-{\\bf b}^T({\\bf x}^{(k)}+\\alpha_k{\\bf r}^{(k)})$$\n",
    "Taking derivative with respect to $\\alpha_k$, we get \n",
    "$$\\alpha_k{\\bf r}^{(k)T}A{\\bf r}^{(k)}+{\\bf r}^{(k)T}A{\\bf x}^{(k)}-{\\bf r}^{(k)T}{\\bf b}=0$$\n",
    "This leads us to \n",
    "$$\\alpha_k=\\dfrac{{\\bf r}^{(k)T}{\\bf r}^{(k)}}{{\\bf r}^{(k)T}A{\\bf r}^{(k)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2230cb94-3b76-4f07-ae4d-9a2196917751",
   "metadata": {},
   "source": [
    "The **Conjugatet Gradient Method** takes the search direction ${\\bf p}^{(k)}$ no longer the same as the residual vector ${\\bf r}^{(k)}$, but the search direction ${\\bf p}^{(k)}$ needs to be perpendicular to all the previous search directions ${\\bf p}^{(j)}$ for $j<k$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a225b-143e-40c5-8132-cd290d81c2e2",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(0, 256, 0, 0.1); padding:10px 0;font-family:monospace;\">\n",
    "    <b>Algorithm: Conjugate Gradient Method.</b><br><hr>\n",
    "    Given an initial guess ${\\bf x}_0$ and a tolerance $\\epsilon$, set at first ${\\bf r}_0={\\bf b}=-A{\\bf x}_0$, $\\delta_0=\\langle {\\bf r}_0, {\\bf r}_0\\rangle$, ${\\bf b}_{\\delta} = \\langle {\\bf b}_0, {\\bf b}_0\\rangle$, $k=0$, and ${\\bf p}_0={\\bf r}_0$. Then:<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; while $\\delta_k>\\epsilon^2{\\bf b}_{\\delta}$:<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; ${\\bf s}_k=A{\\bf p}_k$<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; $\\alpha_k = \\dfrac{\\delta_k}{\\langle {\\bf p}_k, {\\bf s}_k\\rangle}$<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; ${\\bf x}_{k+1} = {\\bf x}_{k}+\\alpha_k{\\bf p}_k$<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; ${\\bf r}_{k+1} = {\\bf r}_{k}-\\alpha_k{\\bf s}_k$<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; $\\delta_{k+1} = \\langle {\\bf r}_{k+1}, {\\bf r}_{k+1}\\rangle$<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; ${\\bf p}_{k+1} = {\\bf r}_{k+1}+\\dfrac{\\delta_{k+1}}{\\delta_{k}} {\\bf p}_{k}$<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; $k = k+1$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93eab86-cf14-4983-9a78-ad95a13127f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
